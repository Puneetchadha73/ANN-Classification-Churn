{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "995d803f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8153 - loss: 0.4428 - val_accuracy: 0.8590 - val_loss: 0.3511\n",
      "Epoch 2/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8533 - loss: 0.3556 - val_accuracy: 0.8590 - val_loss: 0.3423\n",
      "Epoch 3/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8623 - loss: 0.3362 - val_accuracy: 0.8470 - val_loss: 0.3731\n",
      "Epoch 4/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8635 - loss: 0.3386 - val_accuracy: 0.8595 - val_loss: 0.3442\n",
      "Epoch 5/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8618 - loss: 0.3410 - val_accuracy: 0.8520 - val_loss: 0.3498\n",
      "Epoch 6/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8605 - loss: 0.3399 - val_accuracy: 0.8570 - val_loss: 0.3496\n",
      "Epoch 7/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8629 - loss: 0.3350 - val_accuracy: 0.8570 - val_loss: 0.3515\n",
      "Epoch 8/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8642 - loss: 0.3356 - val_accuracy: 0.8640 - val_loss: 0.3453\n",
      "Epoch 9/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8687 - loss: 0.3305 - val_accuracy: 0.8555 - val_loss: 0.3459\n",
      "Epoch 10/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8705 - loss: 0.3233 - val_accuracy: 0.8475 - val_loss: 0.3782\n",
      "Epoch 11/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8645 - loss: 0.3236 - val_accuracy: 0.8605 - val_loss: 0.3552\n",
      "Epoch 12/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8686 - loss: 0.3183 - val_accuracy: 0.8545 - val_loss: 0.3530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 61170), started 1 day, 0:47:39 ago. (Use '!kill 61170' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a85c886bcc0d2600\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a85c886bcc0d2600\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## This is being done in nltk-env (Python 3.11.4)\n",
    "## ANN implementation\n",
    "## libraries\n",
    "## tensorflow ! pip install tensorflow\n",
    "## pandas ! pip install pandas\n",
    "## numpy ! pip install numpy\n",
    "## scikit-learn ! pip install scikit-learn\n",
    "## tensorboard ! pip install tensorboard\n",
    "## matplotlib ! pip install matplotlib\n",
    "## streamlit ! pip install streamlit\n",
    "\n",
    "## ! pip install tensorflow pandas numpy scikit-learn tensorboard matplotlib streamlit\n",
    "\n",
    "## Load the dataset--> drop irrelevant columns--> Encode categorical variables(Gender-->LabelEncoder, Geography-->OHE)--> Combine OHE with original data --> \n",
    "## --> Save encoders as pickle files --> Divide the dataset into independent and dependant features --> Split the data in training and testing sets -->\n",
    "## --> Scale these features\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pickle\n",
    "\n",
    "## Load the dataset\n",
    "data=pd.read_csv('/Users/puneetch/Desktop/Python/basics/churn_modelling.csv')\n",
    "data.head()\n",
    "\n",
    "## Pre-process the data\n",
    "## Drop irrelevant columns\n",
    "\n",
    "data=data.drop(['RowNumber','CustomerId','Surname'],axis=1)\n",
    "\n",
    "## Encode categorical variable\n",
    "label_encoder_gender=LabelEncoder()\n",
    "data['Gender']=label_encoder_gender.fit_transform(data['Gender'])\n",
    "\n",
    "## One Hot encode Geography column\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder_geo=OneHotEncoder(sparse_output=False)\n",
    "geo_encoder=onehot_encoder_geo.fit_transform(data[['Geography']])\n",
    "## onehot_encoder_geo.get_feature_names_out(['Geography'])\n",
    "\n",
    "geo_encoded_df=pd.DataFrame(geo_encoder,columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\n",
    "\n",
    "## Combine One Hot Encoded columns with the original data\n",
    "geo_rem=data.drop(['Geography'],axis=1)\n",
    "data=pd.concat([geo_rem,geo_encoded_df],axis=1)\n",
    "data.head()\n",
    "\n",
    "\n",
    "## save the encoders and sscalar\n",
    "\n",
    "with open('label_encoder_gender.pkl','wb') as file:\n",
    "    pickle.dump(label_encoder_gender,file)\n",
    "\n",
    "with open('onehot_encoder_geo.pkl','wb') as file:\n",
    "    pickle.dump(onehot_encoder_geo,file)\n",
    "\n",
    "\n",
    "## Divide the dataset into independent and dependant features\n",
    "## Exited is dependent and rest all are independent\n",
    "\n",
    "x=data.drop('Exited',axis=1)\n",
    "y=data['Exited']\n",
    "\n",
    "## Split the data in training and testing sets\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "\n",
    "## Scale these features\n",
    "scaler=StandardScaler()\n",
    "x_train=scaler.fit_transform(x_train)\n",
    "x_test=scaler.transform(x_test)\n",
    "\n",
    "## Save it as a pickle file\n",
    "\n",
    "with open('sscaler.pkl','wb') as file:\n",
    "    pickle.dump(scaler,file)\n",
    "\n",
    "\n",
    "## Build our ANN model\n",
    "## Sequential n/w -->Dense for neurons --> Activation function(Sigmoid, tanh, relu, leaky relu) --> Optimizer(useful for backward propogation, updating the weights) -->\n",
    "## --> loss function--> metrics(accuracy) --> Training(Store logs in a folder) --> Tensorboard for visualization\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import datetime\n",
    "\n",
    "## x_train.shape[1]\n",
    "## Creating the model\n",
    "model = Sequential([\n",
    "    Input(shape=(x_train.shape[1],)), # Use Input() as the first layer\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "## compile the model - in oder to do forward and backward propogation\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "losses=tf.keras.losses.BinaryCrossentropy()\n",
    "model.compile(optimizer=opt,loss=losses,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "## Setup the tensorboard\n",
    "logs=\"logs/fit\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorflow_callback=TensorBoard(log_dir=logs,histogram_freq=1)\n",
    "\n",
    "## Setup Early Stopping\n",
    "early_stopping_callback=EarlyStopping(monitor='val_loss',patience=10,restore_best_weights=True)\n",
    "\n",
    "## Training the model\n",
    "history=model.fit(\n",
    "\n",
    "    x_train,y_train,validation_data=(x_test,y_test),epochs=100,\n",
    "    callbacks=[tensorflow_callback,early_stopping_callback]\n",
    ")\n",
    "\n",
    "model.save('model.h5')\n",
    "\n",
    "## Laod tensorflow extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit20250606-223315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372bf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ff7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66308b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39bc30d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4051989f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0393fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4db19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e1b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4d579680",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANN 1st step practice\n",
    "\n",
    "##! pip install tensorflow pandas numpy scikit-learn tensorboard matplotlib streamlit\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pickle\n",
    "\n",
    "## Read csv\n",
    "data=pd.read_csv(\"/Users/puneetch/Desktop/Python/basics/churn_modelling.csv\")\n",
    "\n",
    "## Remove unwanted columns\n",
    "data=data.drop(['RowNumber','CustomerId','Surname'],axis=1)\n",
    "\n",
    "## Encode Gender to 0 or 1\n",
    "label_encoder_gender=LabelEncoder()\n",
    "data['Gender']=label_encoder_gender.fit_transform(data['Gender'])\n",
    "\n",
    "## Encode Country using One Hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder_geo=OneHotEncoder(sparse_output=False)\n",
    "geo_encoder=onehot_encoder_geo.fit_transform(data[['Geography']])\n",
    "## onehot_encoder_geo.get_feature_names_out(['Geography'])\n",
    "\n",
    "geo_encoded_df=pd.DataFrame(geo_encoder,columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\n",
    "\n",
    "## Combine One Hot Encoded columns with the original data\n",
    "data=pd.concat((data.drop(['Geography'],axis=1),geo_encoded_df),axis=1)\n",
    "\n",
    "## save the encoders as pickle files\n",
    "with open('label_encoder_gender.pkl','wb') as file:\n",
    "    pickle.dump(label_encoder_gender,file)\n",
    "\n",
    "with open('onehot_encoder_geo.pkl','wb') as file:\n",
    "    pickle.dump(label_encoder_gender,file)\n",
    "\n",
    "## Divide the dataset into independent and dependant features\n",
    "## Exited is dependent and rest all are independent\n",
    "\n",
    "x=data.drop('Exited',axis=1)\n",
    "y=data['Exited']\n",
    "\n",
    "## Split the data in training and testing sets\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "\n",
    "## Scale these features\n",
    "scaler=StandardScaler()\n",
    "x_train=scaler.fit_transform(x_train)\n",
    "x_test=scaler.fit(x_test)\n",
    "\n",
    "## Save it as a pickle file\n",
    "with open('scaler.pkl','wb') as file:\n",
    "    pickle.dump(scaler,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cc1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec523aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce285e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78a1b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232cf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fbc4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bdf149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5fed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b3417d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nltk-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
